{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.proportion import multinomial_proportions_confint as mpc\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from helper_functions import flatten_list, reverse_dict_list, reverse_dict_val, sorted_dict, loadPKL, set_xticks, set_yticks, SENT2IDX, SENT2LAB, stats, pval_star, print_sigfig\n",
    "\n",
    "\n",
    "CWD = os.path.abspath(\"\")  # Jupyter notebook path.\n",
    "dir_TEMP = os.path.join(CWD, \"TEMP\")  # Intermediate files.\n",
    "dir_dict = os.path.join(CWD, \"dicts\")  # Data to plot.\n",
    "dir_input = os.path.join(CWD, \"input\")\n",
    "dir_output = os.path.join(CWD, \"output\")  # Folder to put figures in.\n",
    "dir_npy = os.path.join(CWD, \"npy\")  # Data files needed for plotting figures.\n",
    "\n",
    "# Set up plotting parameters.\n",
    "cm_max = 20  # Maximum number of citation mentions (AKA citation frequency) to plot.\n",
    "sent_colors = [\"#504DB2\", \"#414042\", \"#B2504D\"]  # POS, NEU, NEG\n",
    "font_kw = {\"family\": \"arial\", \"weight\": \"normal\", \"size\": \"7\"}  # Markers and such.\n",
    "mpl.rc(\"font\", **font_kw)\n",
    "\n",
    "# Load data to plot.\n",
    "with open(os.path.join(dir_TEMP, \"sentences2rate-CGPT.txt\"), mode=\"r+\", encoding=\"UTF-8\") as file_out:\n",
    "    sentences2rate = file_out.readlines()\n",
    "cite2sent_0 = loadPKL(dir_TEMP, \"cite2sent_0\")  # Pre-aggregation citation sentences indices.\n",
    "cite2sent_1 = loadPKL(dir_TEMP, \"cite2sent_1\")  # Pre-aggregation citation sentiment.\n",
    "cite2ns = loadPKL(dir_TEMP, \"cite2ns\")  # Citation frequency between a given citer-citee pair.\n",
    "cite2sent_emp = loadPKL(dir_dict, \"cite2sent_2\")  # Each citation pair has just 1 sentiment; \"empirical\".\n",
    "paper2meta = loadPKL(dir_dict, \"paper2meta\")\n",
    "jour2meta = loadPKL(dir_dict, \"jour2meta\")\n",
    "cite2distance = loadPKL(dir_dict, \"cite2distance\")  # Collaboration distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h-Index stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_dict, \"last_author2hIndex.pkl\"), \"rb\") as f:\n",
    "    last_author2hIndex = pickle.load(f)\n",
    "with open(os.path.join(dir_dict, \"paper2last_author.pkl\"), \"rb\") as f:\n",
    "    paper2last_author = pickle.load(f)\n",
    "\n",
    "\n",
    "last_authors = {au for au in paper2last_author.values()}\n",
    "presence = {au for au in last_authors if au in last_author2hIndex}\n",
    "a = len(presence) / len(last_authors) * 100\n",
    "print(f\"Num of last authors: {len(last_authors)} {a:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab groups aggregate behavior stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 collab distance types; [1, inf), [1], [2, inf), [0].\n",
    "ratio_mat_rel = np.load(os.path.join(dir_npy, \"ratio_mat_rel-collab_groups.npy\"))\n",
    "\n",
    "print(\"two-sided Welch's $t$-test:\\n\")\n",
    "for s, i in SENT2IDX.items():\n",
    "    print(f\"{SENT2LAB[s]}\")\n",
    "    abc1 = np.mean(ratio_mat_rel[1, i, :])\n",
    "    abc2 = np.std(ratio_mat_rel[1, i, :], ddof=1)\n",
    "    abc3 = np.mean(ratio_mat_rel[2, i, :])\n",
    "    abc4 = np.std(ratio_mat_rel[2, i, :], ddof=1)\n",
    "    res = stats.ttest_ind(ratio_mat_rel[1, i, :], ratio_mat_rel[2, i, :], equal_var=False, alternative=\"two-sided\")\n",
    "    txt_box0 = \"\"\n",
    "    txt_box0 += f\"t({print_sigfig(res.df)})={print_sigfig(res.statistic)}, {pval_star(res.pvalue)}\"\n",
    "    txt_box0 += f\", collab-non-collab diff {print_sigfig((abc1-abc3))}, 95% CI ({print_sigfig(res.confidence_interval().low)},{print_sigfig(res.confidence_interval().high)})\"\n",
    "    txt_box0 += f\"\\ncollab {print_sigfig(abc1)}±{print_sigfig(abc2)}\\nnon-collab {print_sigfig(abc3)}±{print_sigfig(abc4)}\\n\"\n",
    "    print(txt_box0, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate pre-aggregation sentiment ratio (of citer-citee pairs) for research, review, and both types of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_POS = sum([sum([x == 1 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_NEU = sum([sum([x == 0 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_NEG = sum([sum([x == -1 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_tot = research_POS + research_NEU + research_NEG\n",
    "review_POS = sum([sum([x == 1 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_NEU = sum([sum([x == 0 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_NEG = sum([sum([x == -1 for x in s]) for e, s in cite2sent_1.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_tot = review_POS + review_NEU + review_NEG\n",
    "x_POS = sum([sum([x == 1 for x in s]) for e, s in cite2sent_1.items()])\n",
    "x_NEU = sum([sum([x == 0 for x in s]) for e, s in cite2sent_1.items()])\n",
    "x_NEG = sum([sum([x == -1 for x in s]) for e, s in cite2sent_1.items()])\n",
    "x_tot = x_POS + x_NEU + x_NEG\n",
    "\n",
    "ratios = [\n",
    "    [research_POS / research_tot * 100, review_POS / review_tot * 100, x_POS / x_tot * 100],\n",
    "    [research_NEU / research_tot * 100, review_NEU / review_tot * 100, x_NEU / x_tot * 100],\n",
    "    [research_NEG / research_tot * 100, review_NEG / review_tot * 100, x_NEG / x_tot * 100],\n",
    "]\n",
    "for r, s in zip(ratios, [\"POS\", \"NEU\", \"NEG\"]):\n",
    "    print(f\"{s} ratios: research {r[0]:.2f}% review {r[1]:.2f}% both {r[2]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate post-aggregation sentiment ratio (of citer-citee pairs) for research, review, and both types of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_POS = sum([s == 1 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_NEU = sum([s == 0 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_NEG = sum([s == -1 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"research-article\"])\n",
    "research_tot = research_POS + research_NEU + research_NEG\n",
    "review_POS = sum([s == 1 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_NEU = sum([s == 0 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_NEG = sum([s == -1 for e, s in cite2sent_emp.items() if paper2meta[e[0]][\"article-type\"] == \"review-article\"])\n",
    "review_tot = review_POS + review_NEU + review_NEG\n",
    "x_POS = sum([s == 1 for e, s in cite2sent_emp.items()])\n",
    "x_NEU = sum([s == 0 for e, s in cite2sent_emp.items()])\n",
    "x_NEG = sum([s == -1 for e, s in cite2sent_emp.items()])\n",
    "x_tot = x_POS + x_NEU + x_NEG\n",
    "\n",
    "ratios = [\n",
    "    [research_POS / research_tot * 100, review_POS / review_tot * 100, x_POS / x_tot * 100],\n",
    "    [research_NEU / research_tot * 100, review_NEU / review_tot * 100, x_NEU / x_tot * 100],\n",
    "    [research_NEG / research_tot * 100, review_NEG / review_tot * 100, x_NEG / x_tot * 100],\n",
    "]\n",
    "for r, s in zip(ratios, [\"POS\", \"NEU\", \"NEG\"]):\n",
    "    print(f\"{s} ratios: research {r[0]:.2f}% review {r[1]:.2f}% both {r[2]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make journal table (only ones ended up in the analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of journals in the data.\n",
    "journals_s = {v[\"jyf\"][0] for v in paper2meta.values()}\n",
    "# jour2meta has all 185+3 considered and 184+3=187 found in Medline.\n",
    "# In final data, there are 181 journals.\n",
    "print(len(journals_s), len(jour2meta))\n",
    "\n",
    "for k in jour2meta:\n",
    "    if jour2meta[k][\"MedAbbr\"] not in journals_s:\n",
    "        # print(jour2meta[k][\"MedAbbr\"], jour2meta[k][\"jourMeta\"][\"OAGoldPercent\"])\n",
    "        pass\n",
    "\n",
    "# Find papers in each journal.\n",
    "jour2paper = {j: set() for j in journals_s}\n",
    "for p, v in paper2meta.items():\n",
    "    jour2paper[v[\"jyf\"][0]].add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print journal table.\n",
    "jour2meta_sorted = dict(\n",
    "    sorted(dict(i for i in jour2meta.items() if i[1][\"MedAbbr\"] in journals_s).items(), key=lambda kv: kv[1][\"JournalTitle\"].casefold())\n",
    ")\n",
    "\n",
    "\n",
    "print(max([len(v[\"JournalTitle\"]) for v in jour2meta_sorted.values()]))\n",
    "print(f\"{'Journal Title':<132s}{'JIF':>10s}{'% OA Gold':>12s}{'Number of Papers in Analysis':>30s}\")\n",
    "bb = [None] * len(jour2meta_sorted)\n",
    "cc = [None] * len(jour2meta_sorted)\n",
    "dd = [None] * len(jour2meta_sorted)\n",
    "for i, k in enumerate(jour2meta_sorted):\n",
    "    a = jour2meta_sorted[k][\"JournalTitle\"]\n",
    "    b = jour2meta_sorted[k][\"jourMeta\"][\"JIF2022\"]\n",
    "    c = jour2meta_sorted[k][\"jourMeta\"][\"OAGoldPercent\"]\n",
    "    d = len(jour2paper[jour2meta_sorted[k][\"MedAbbr\"]])\n",
    "    bb[i] = b\n",
    "    cc[i] = c\n",
    "    dd[i] = d\n",
    "    print(f\"{a:<132s}{b:>10}{c:>12}{d:>30}\")\n",
    "print(f\"{'Journal Average':<132s}{np.mean(bb):>10.1f}{np.mean(cc):>12.2f}{np.mean(dd):>30.3f}\")\n",
    "print(f\"{'Journal Standard Deviation':<132s}{np.std(bb, ddof=1):>10.1f}{np.std(cc, ddof=1):>12.2f}{np.std(dd, ddof=1):>30.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make department table (all 28 considered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dir_dict, \"paper2last_author.pkl\"), \"rb\") as f:\n",
    "    paper2last_author = pickle.load(f)\n",
    "with open(os.path.join(dir_dict, \"paper2first_author.pkl\"), \"rb\") as f:\n",
    "    paper2first_author = pickle.load(f)\n",
    "last_author2paper = reverse_dict_val(paper2last_author)\n",
    "first_author2paper = reverse_dict_val(paper2first_author)\n",
    "\n",
    "with open(os.path.join(dir_dict, \"paper2last_author_department_28_dep.pkl\"), \"rb\") as f:\n",
    "    paper2last_author_department = pickle.load(f)  # Last author departments for each paper.\n",
    "with open(os.path.join(dir_dict, \"paper2first_author_department_28_dep.pkl\"), \"rb\") as f:\n",
    "    paper2first_author_department = pickle.load(f)  # First author departments for each paper.\n",
    "last_author_department2paper = reverse_dict_list(paper2last_author_department)\n",
    "first_author_department2paper = reverse_dict_list(paper2first_author_department)\n",
    "dep_names = sorted(list(last_author_department2paper.keys()))\n",
    "\n",
    "\n",
    "last_author_dep2paper = reverse_dict_list(paper2last_author_department)\n",
    "last_author2dep = {au: [] for au in last_author2paper}\n",
    "for p, colist in paper2last_author_department.items():\n",
    "    last_author2dep[paper2last_author[p]] += colist\n",
    "last_author2dep = {au: list(set(colist)) for au, colist in last_author2dep.items()}\n",
    "dep2last_author = reverse_dict_list(last_author2dep)\n",
    "\n",
    "first_author_dep2paper = reverse_dict_list(paper2first_author_department)\n",
    "first_author2dep = {au: [] for au in first_author2paper}\n",
    "for p, colist in paper2first_author_department.items():\n",
    "    first_author2dep[paper2first_author[p]] += colist\n",
    "first_author2dep = {au: list(set(colist)) for au, colist in first_author2dep.items()}\n",
    "dep2first_author = reverse_dict_list(first_author2dep)\n",
    "\n",
    "last_author_dep2n_paper = dict(sorted({co: len(pp) for co, pp in last_author_dep2paper.items()}.items(), key=lambda x: x[0]))\n",
    "first_author_dep2n_paper = dict(sorted({co: len(pp) for co, pp in first_author_dep2paper.items()}.items(), key=lambda x: x[0]))\n",
    "\n",
    "dep2n_last_author = dict(sorted({co: len(pp) for co, pp in dep2last_author.items()}.items(), key=lambda x: x[0]))\n",
    "dep2n_first_author = dict(sorted({co: len(pp) for co, pp in dep2first_author.items()}.items(), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{'Department':<45s}{'# of Last Authors':>20s}{'# of Last Author Papers':>30s}{'# of First Authors':>25s}{'# of First Author Papers':>30s}\"\n",
    ")\n",
    "aa, bb, cc, dd = [None] * len(dep_names), [None] * len(dep_names), [None] * len(dep_names), [None] * len(dep_names)\n",
    "for i, co in enumerate(dep_names):\n",
    "    a = dep2n_last_author[co] if co in dep2n_last_author else 0\n",
    "    b = last_author_dep2n_paper[co] if co in last_author_dep2n_paper else 0\n",
    "    c = dep2n_first_author[co] if co in dep2n_first_author else 0\n",
    "    d = first_author_dep2n_paper[co] if co in first_author_dep2n_paper else 0\n",
    "    aa[i], bb[i], cc[i], dd[i] = a, b, c, d\n",
    "    print(f\"{co.capitalize():<45s}{a:>20}{b:>30}{c:>25}{d:>30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make country table (27 that end up in the analyses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_list import countries_for_language\n",
    "\n",
    "two_letter2full_name = dict(countries_for_language(\"en\"))\n",
    "\n",
    "with open(os.path.join(dir_dict, \"country2power_distance.pkl\"), \"rb\") as f:\n",
    "    country2power_distance = pickle.load(f)\n",
    "with open(os.path.join(dir_dict, \"country2individualism.pkl\"), \"rb\") as f:\n",
    "    country2individualism = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(dir_dict, \"paper2last_author.pkl\"), \"rb\") as f:\n",
    "    paper2last_author = pickle.load(f)\n",
    "with open(os.path.join(dir_dict, \"paper2first_author.pkl\"), \"rb\") as f:\n",
    "    paper2first_author = pickle.load(f)\n",
    "last_author2paper = reverse_dict_val(paper2last_author)\n",
    "first_author2paper = reverse_dict_val(paper2first_author)\n",
    "\n",
    "with open(os.path.join(dir_dict, \"paper2last_author_country.pkl\"), \"rb\") as f:\n",
    "    paper2last_author_country = pickle.load(f)  # Last author countries for each paper.\n",
    "with open(os.path.join(dir_dict, \"paper2first_author_country.pkl\"), \"rb\") as f:\n",
    "    paper2first_author_country = pickle.load(f)  # First author countries for each paper.\n",
    "last_author_country2paper = reverse_dict_list(paper2last_author_country)\n",
    "first_author_country2paper = reverse_dict_list(paper2first_author_country)\n",
    "country_names = sorted(list(country2power_distance.keys()), key=lambda x: two_letter2full_name[x])\n",
    "\n",
    "\n",
    "last_author_nat2paper = reverse_dict_list(paper2last_author_country)\n",
    "last_author2nat = {au: [] for au in last_author2paper}\n",
    "for p, colist in paper2last_author_country.items():\n",
    "    last_author2nat[paper2last_author[p]] += colist\n",
    "last_author2nat = {au: list(set(colist)) for au, colist in last_author2nat.items()}\n",
    "nat2last_author = reverse_dict_list(last_author2nat)\n",
    "\n",
    "first_author_nat2paper = reverse_dict_list(paper2first_author_country)\n",
    "first_author2nat = {au: [] for au in first_author2paper}\n",
    "for p, colist in paper2first_author_country.items():\n",
    "    first_author2nat[paper2first_author[p]] += colist\n",
    "first_author2nat = {au: list(set(colist)) for au, colist in first_author2nat.items()}\n",
    "nat2first_author = reverse_dict_list(first_author2nat)\n",
    "\n",
    "last_author_nat2n_paper = dict(sorted({co: len(pp) for co, pp in last_author_nat2paper.items()}.items(), key=lambda x: x[0]))\n",
    "first_author_nat2n_paper = dict(sorted({co: len(pp) for co, pp in first_author_nat2paper.items()}.items(), key=lambda x: x[0]))\n",
    "\n",
    "nat2n_last_author = dict(sorted({co: len(pp) for co, pp in nat2last_author.items()}.items(), key=lambda x: x[0]))\n",
    "nat2n_first_author = dict(sorted({co: len(pp) for co, pp in nat2first_author.items()}.items(), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{'Country/Region':<25s}{'Power Distance':>20s}{'Individualism':>20s}{'# of Last Authors':>20s}{'# of Last Author Papers':>30s}{'# of First Authors':>25s}{'# of First Author Papers':>30s}\"\n",
    ")\n",
    "for i, co in enumerate(country_names):\n",
    "    a = nat2n_last_author[co] if co in nat2n_last_author else 0\n",
    "    b = last_author_nat2n_paper[co] if co in last_author_nat2n_paper else 0\n",
    "    c = nat2n_first_author[co] if co in nat2n_first_author else 0\n",
    "    d = first_author_nat2n_paper[co] if co in first_author_nat2n_paper else 0\n",
    "    pwd = int(country2power_distance[co])\n",
    "    inv = int(country2individualism[co])\n",
    "    full_name = f\"{two_letter2full_name[co]} ({co})\"\n",
    "    print(f\"{full_name:<25s}{pwd:>20}{inv:>20}{a:>20}{b:>30}{c:>25}{d:>30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make figure: sentiment ratio (post-hierarchy) as a function of number of sentences in pre-hierarchy.\n",
    "\n",
    "#### First is monte carlo simulation using pre-hierarchy sentiment ratio.\n",
    "#### Second is empirical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment ratio prevalence (pre-hierarchy)\n",
    "p_neg = 0.0602\n",
    "p_pos = 0.2488\n",
    "p_neu = 0.6910\n",
    "n_mc = 1000\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "ratio_mat_rel = np.zeros((cm_max, 3, n_mc))\n",
    "n_cp = np.zeros(cm_max)  # Num of citation pairs.\n",
    "\"\"\"\n",
    "for a given ns (num of sentence):\n",
    "1. In a citation pair, we have ns num of sentences, each sentence we draw a sentiment from a categorical distribution of using overall prevalence data above. In other words we draw from categorical distribution ns times. Do hierarchy (AKA aggregation) to calculate post-hierarchy sentiment for this pair.\n",
    "2. Do that for all num of citation pairs; we use the empirical num.\n",
    "3. Calculate post-hierarchy sentiment ratio for each sentiment among these sentimented pairs.\n",
    "4. Repeat step 1-3 n_mc times.\n",
    "\n",
    "We get a distribution of post-hierarchy sentiment ratio for a given ns. Use mean+sem or whatever and plot.\n",
    "\"\"\"\n",
    "\n",
    "for n_sentence in tqdm(range(cm_max)):\n",
    "    n_pair = len([None for pair, ns in cite2ns.items() if ns == n_sentence + 1])\n",
    "    n_cp[n_sentence] = n_pair\n",
    "    # continue  # Skip simulation.\n",
    "    sent_ = rng.choice([1, 0, -1], p=[p_pos, p_neu, p_neg], size=(n_pair, n_sentence, n_mc), replace=True)\n",
    "    # Perform hierarchy aggregation along n_sentence dim: -1 > 1 > 0.\n",
    "    sent_ = np.where((sent_ == -1).any(1), -1, np.where((sent_ == 1).any(1), 1, 0))\n",
    "    # Calculate post-hierarchy sentiment ratio along n_pair dim.\n",
    "    ratio_mat_rel[n_sentence, :, :] = np.vstack([(sent_ == v).mean(0) for v in [1, 0, -1]])  # dim=(3, n_mc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure production.\n",
    "fig, ax = plt.subplots(figsize=(3.41, 3.41))\n",
    "xticklabels = [f\"{x}\" for x in range(0, cm_max + 1)]  # Citation frequency label.\n",
    "x_arr = np.arange(len(xticklabels))\n",
    "med = np.median(np.array(list(cite2ns.values())))\n",
    "p95 = np.percentile(np.array(list(cite2ns.values())), 95)\n",
    "\n",
    "\n",
    "ax.vlines(med, ymin=1, ymax=4e5, color=\"red\", alpha=0.5, zorder=1, linestyle=\":\")\n",
    "ax.vlines(p95, ymin=1, ymax=4e5, color=\"red\", alpha=0.5, zorder=1, linestyle=\"-\")\n",
    "ax.plot([t2 for t1, t2 in enumerate(x_arr) if t1 != 0], n_cp, color=\"grey\", alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(\"Citation Frequency\", size=10)  # x-large\n",
    "ax.set_ylabel(\"Number of Citation Pairs\", size=10)  # x-large\n",
    "ax.set_xticks(x_arr, xticklabels)\n",
    "ax.grid(which=\"major\", axis=\"x\", alpha=0.2)\n",
    "ax.grid(which=\"major\", axis=\"y\", alpha=0.2)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([1, 4e5])\n",
    "set_xticks(ax)\n",
    "set_yticks(ax)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(dir_output, \"SUPP Sample Size vs. NS.svg\"), bbox_inches=\"tight\", transparent=True)\n",
    "fig.clf()  # Clear figure.\n",
    "plt.close(fig=fig)  # Close figure.\n",
    "\n",
    "print(f\"citation frequency median: {med}\")\n",
    "print(f\"citation frequency 95 percentile: {p95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure production.\n",
    "fig, ax = plt.subplots(figsize=(3.41, 3.41))\n",
    "xticklabels = [f\"{x}\" for x in range(0, cm_max + 1)]  # Citation frequency label.\n",
    "x_arr = np.arange(len(xticklabels))\n",
    "# Baseline (indistinguishable from null).\n",
    "ax.plot(x_arr, [0 for _ in x_arr], color=\"grey\", alpha=0.5, zorder=1, linestyle=\":\")\n",
    "\n",
    "for i in range(3):  # One curve for each of the 3 sentiments.\n",
    "    m = np.nanmean(ratio_mat_rel[:, i, :], axis=-1)  # From bootstrap sampling distribution.\n",
    "    std = np.nanstd(ratio_mat_rel[:, i, :], axis=-1, ddof=1)  # From bootstrap sampling distribution.\n",
    "    ax.plot([t2 for t1, t2 in enumerate(x_arr) if t1 != 0], m, color=sent_colors[i])\n",
    "    ax.fill_between([t2 for t1, t2 in enumerate(x_arr) if t1 != 0], m - std, m + std, color=sent_colors[i], alpha=0.3, edgecolor=None)\n",
    "\n",
    "ax.set_xlabel(\"Citation Frequency\", size=10)  # x-large\n",
    "ax.set_ylabel(\"Sentiment Ratio\", size=10)  # x-large\n",
    "ax.set_xticks(x_arr, xticklabels)\n",
    "ax.grid(which=\"major\", axis=\"x\", alpha=0.2)\n",
    "ax.grid(which=\"major\", axis=\"y\", alpha=0.2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(dir_output, \"SUPP SR vs. NS (Monte Carlo Simulation).svg\"), bbox_inches=\"tight\", transparent=True)\n",
    "fig.clf()  # Clear figure.\n",
    "plt.close(fig=fig)  # Close figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is empirical plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row: Citatino frequency; col: 3 sentiment\n",
    "def _is_goodman_invalid(counts, th=5):\n",
    "    for c in counts:\n",
    "        if c < th:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "count_mat_rel = np.zeros((cm_max, 3))\n",
    "ratio_mat_rel = np.zeros((cm_max, 3))\n",
    "for d in range(cm_max):\n",
    "    sent_emp = np.array([cite2sent_emp[pair] for pair, ns in cite2ns.items() if ns == d + 1])\n",
    "    r_emp = np.array([np.sum(sent_emp == s) for s in [1, 0, -1]], dtype=float)\n",
    "    count_mat_rel[d, :] = r_emp\n",
    "    r_emp /= np.sum(r_emp)\n",
    "    ratio_mat_rel[d, :] = r_emp\n",
    "\n",
    "\n",
    "count_mat_rel_pre = np.zeros((cm_max, 3))\n",
    "ratio_mat_rel_pre = np.zeros((cm_max, 3))\n",
    "for d in range(cm_max):\n",
    "    sent_emp = np.array(flatten_list([cite2sent_1[pair] for pair, ns in cite2ns.items() if ns == d + 1]))\n",
    "    r_emp = np.array([np.sum(sent_emp == s) for s in [1, 0, -1]], dtype=float)\n",
    "    count_mat_rel_pre[d, :] = r_emp\n",
    "    r_emp /= np.sum(r_emp)\n",
    "    ratio_mat_rel_pre[d, :] = r_emp\n",
    "\n",
    "ymin, ymax = -0.02, 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure production.\n",
    "fig, ax = plt.subplots(figsize=(3.41 * 1.5, 3.41))\n",
    "xticklabels = [f\"{x}\" for x in range(0, cm_max + 1)]  # Citation frequency label.\n",
    "x_arr = np.arange(1, len(xticklabels))\n",
    "\n",
    "for i in range(3):  # One curve for each of the 3 sentiments.\n",
    "    ax.plot(x_arr, ratio_mat_rel[:, i], color=sent_colors[i])\n",
    "    cfy = np.array([mpc(count_mat_rel[j - 1, :], alpha=0.05, method=\"goodman\")[i, :] for j in x_arr])\n",
    "    ax.fill_between(x_arr, cfy[:, 0], cfy[:, 1], color=sent_colors[i], alpha=0.3, edgecolor=None)\n",
    "for xi in x_arr[np.array([_is_goodman_invalid(count_mat_rel[j - 1, :], 10) for j in x_arr])]:\n",
    "    xmin = xi - 0.5\n",
    "    xmax = xi + 0.5 if xi != x_arr[-1] else xi\n",
    "    ax.hlines(y=0.11, xmin=xmin, xmax=xmax, color=\"black\", linewidth=2, linestyle=(0, (4, 4)))\n",
    "for xi in x_arr[np.array([_is_goodman_invalid(count_mat_rel[j - 1, :], 5) for j in x_arr])]:\n",
    "    xmin = xi - 0.5\n",
    "    xmax = xi + 0.5 if xi != x_arr[-1] else xi\n",
    "    ax.hlines(y=0.18, xmin=xmin, xmax=xmax, color=\"black\", linewidth=2, linestyle=(0, (4, 4)))\n",
    "ax.text(18, 0.112, \"n < 10\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.text(19, 0.182, \"n < 5\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "x_arr = np.arange(len(xticklabels))\n",
    "\n",
    "ax.set_xlabel(\"Citation Frequency\", size=10)  # x-large\n",
    "ax.set_ylabel(\"Sentiment Ratio\", size=10)  # x-large\n",
    "ax.set_xticks([t2 for t1, t2 in enumerate(x_arr) if t1 % 5 == 0])  # Citation frequency label.\n",
    "ax.set_xticklabels([t2 for t1, t2 in enumerate(xticklabels) if t1 % 5 == 0])  # Citation frequency label.\n",
    "\n",
    "# ymin, ymax = ax.get_ylim()\n",
    "for xi in x_arr:\n",
    "    ax.axvline(x=xi, color=\"lightgray\", linestyle=\"-\", linewidth=0.5, zorder=0)\n",
    "for yi in ax.get_yticks():\n",
    "    ax.axhline(y=yi, color=\"lightgray\", linestyle=\"-\", linewidth=0.5, zorder=0)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlim(x_arr[0] - 0.5, x_arr[-1] + 0.5)\n",
    "\n",
    "major_len = 6.5\n",
    "minor_len = 4\n",
    "major_width = 1.5\n",
    "minor_width = 1\n",
    "# ax.tick_params(axis=\"y\", which=\"major\", length=major_len, width=major_width, labelsize=10)\n",
    "ax.tick_params(axis=\"x\", which=\"major\", length=major_len, width=major_width, labelsize=10)\n",
    "set_yticks(ax, alt=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(dir_output, \"SUPP SR vs. NS.svg\"), bbox_inches=\"tight\", transparent=True)\n",
    "fig.clf()  # Clear figure.\n",
    "plt.close(fig=fig)  # Close figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure production.\n",
    "fig, ax = plt.subplots(figsize=(3.41 * 1.5, 3.41))\n",
    "xticklabels = [f\"{x}\" for x in range(0, cm_max + 1)]  # Citation frequency label.\n",
    "x_arr = np.arange(1, len(xticklabels))\n",
    "\n",
    "for i in range(3):  # One curve for each of the 3 sentiments.\n",
    "    ax.plot(x_arr, ratio_mat_rel_pre[:, i], color=sent_colors[i])\n",
    "    cfy = np.array([mpc(count_mat_rel_pre[j - 1, :], alpha=0.05, method=\"goodman\")[i, :] for j in x_arr])\n",
    "    ax.fill_between(x_arr, cfy[:, 0], cfy[:, 1], color=sent_colors[i], alpha=0.3, edgecolor=None)\n",
    "for xi in x_arr[np.array([_is_goodman_invalid(count_mat_rel_pre[j - 1, :], 10) for j in x_arr])]:\n",
    "    xmin = xi - 0.5\n",
    "    xmax = xi + 0.5 if xi != x_arr[-1] else xi\n",
    "    ax.hlines(y=0.11, xmin=xmin, xmax=xmax, color=\"black\", linewidth=2, linestyle=(0, (4, 4)))\n",
    "for xi in x_arr[np.array([_is_goodman_invalid(count_mat_rel_pre[j - 1, :], 5) for j in x_arr])]:\n",
    "    xmin = xi - 0.5\n",
    "    xmax = xi + 0.5 if xi != x_arr[-1] else xi\n",
    "    ax.hlines(y=0.18, xmin=xmin, xmax=xmax, color=\"black\", linewidth=2, linestyle=(0, (4, 4)))\n",
    "# ax.text(18, 0.112, \"n < 10\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "# ax.text(19, 0.182, \"n < 5\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "x_arr = np.arange(len(xticklabels))\n",
    "\n",
    "ax.set_xlabel(\"Citation Frequency\", size=10)  # x-large\n",
    "ax.set_ylabel(\"Sentiment Ratio\", size=10)  # x-large\n",
    "ax.set_xticks([t2 for t1, t2 in enumerate(x_arr) if t1 % 5 == 0])  # Citation frequency label.\n",
    "ax.set_xticklabels([t2 for t1, t2 in enumerate(xticklabels) if t1 % 5 == 0])  # Citation frequency label.\n",
    "\n",
    "# ymin, ymax = ax.get_ylim()\n",
    "for xi in x_arr:\n",
    "    ax.axvline(x=xi, color=\"lightgray\", linestyle=\"-\", linewidth=0.5, zorder=0)\n",
    "for yi in ax.get_yticks():\n",
    "    ax.axhline(y=yi, color=\"lightgray\", linestyle=\"-\", linewidth=0.5, zorder=0)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlim(x_arr[0] - 0.5, x_arr[-1] + 0.5)\n",
    "\n",
    "major_len = 6.5\n",
    "minor_len = 4\n",
    "major_width = 1.5\n",
    "minor_width = 1\n",
    "# ax.tick_params(axis=\"y\", which=\"major\", length=major_len, width=major_width, labelsize=10)\n",
    "ax.tick_params(axis=\"x\", which=\"major\", length=major_len, width=major_width, labelsize=10)\n",
    "set_yticks(ax, alt=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(dir_output, \"SUPP SR vs. NS (pre-agg).svg\"), bbox_inches=\"tight\", transparent=True)\n",
    "fig.clf()  # Clear figure.\n",
    "plt.close(fig=fig)  # Close figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite2sent = loadPKL(dir_dict, \"cite2sent_2\")\n",
    "\n",
    "paper2num_paper = defaultdict(set)\n",
    "for citer, citee in cite2sent.keys():\n",
    "    paper2num_paper[citer].add(citee)\n",
    "paper2num_paper = {k: len(v) for k, v in paper2num_paper.items()}\n",
    "\n",
    "arr = np.array(list(paper2num_paper.values()))\n",
    "print(f\"sample size: {len(paper2num_paper)} | range(x): {min(arr)}, {max(arr)}\")\n",
    "values = np.arange(1, 21)\n",
    "counts = np.bincount(arr)[1:21]  # skip index 0\n",
    "plt.bar(values, counts)\n",
    "plt.axvline(np.mean(arr), color=\"black\", linestyle=\"--\", linewidth=2, label=f\"Mean\")\n",
    "plt.axvline(np.median(arr), color=\"black\", linestyle=\":\", linewidth=2, label=f\"Median\")\n",
    "plt.xlabel(\"Number of Unique Papers Cited\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xticks(values)\n",
    "plt.savefig(os.path.join(dir_output, \"SUPP Number of Unique Papers Cited.svg\"), format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite2ns = loadPKL(dir_TEMP, \"cite2ns\")\n",
    "\n",
    "arr = np.array(list(cite2ns.values()))\n",
    "print(f\"sample size: {len(cite2ns)} | range(x): {min(arr)}, {max(arr)}\")\n",
    "values = np.arange(1, 21)\n",
    "counts = np.bincount(arr)[1:21]  # skip index 0\n",
    "plt.bar(values, counts)\n",
    "plt.axvline(np.mean(arr), color=\"black\", linestyle=\"--\", linewidth=2, label=f\"Mean\")\n",
    "plt.axvline(np.median(arr), color=\"black\", linestyle=\":\", linewidth=2, label=f\"Median\")\n",
    "plt.xlabel(\"Number of Citation Sentences for a Given Citation Pair\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xticks(values)\n",
    "plt.savefig(os.path.join(dir_output, \"SUPP Number of Citation Sentences for a Given Citation Pair.svg\"), format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_px",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
