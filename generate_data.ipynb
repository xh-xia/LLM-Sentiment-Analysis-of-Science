{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from helper_functions import loadPKL\n",
    "\n",
    "CWD = os.path.abspath(\"\")  # Jupyter notebook path.\n",
    "\n",
    "dir_input = os.path.join(CWD, \"input\")\n",
    "dir_batch = os.path.join(CWD, \"batch\")  # ChatGPT related output.\n",
    "dir_TEMP = os.path.join(CWD, \"TEMP\")  # Intermediate files.\n",
    "dir_dict = os.path.join(CWD, \"dicts\")  # Look up dictionaries such as paper2meta; main data directory.\n",
    "dir_npy = os.path.join(CWD, \"npy\")  # Data files needed for plotting figures.\n",
    "dir_output = os.path.join(CWD, \"output\")  # Figures.\n",
    "dir_xml = os.path.join(CWD, \"xml\")  # xml files.\n",
    "dir_DEBUG = os.path.join(CWD, \"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters json file in /input.\n",
    "\n",
    "TRAINING_CSV_FILE_NAME:</br>\n",
    "csv filename (don't include extension) containing human rated answers; this file should be in /batch.\n",
    "it should have sentences column as well as one column of ratings for each rater; first row is column name, sentence column should be named \"sentences\", rater column names can be whatever; column order doesn't matter, so can be rater1, rater 2, sentences, rater 3, for example\n",
    "sentences column contains the sentence to rate, and rating columns contain ratings for that rater\n",
    "\n",
    "THRES_NUM_PAIR_COLLAB (int):</br>\n",
    "Sample size threshold for at least this amount of citation pairs to collaborators.</br>\n",
    "Used to filter out departments and countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year_range': [1998, 2023],\n",
       " 'jif_thres': 3,\n",
       " 'file_name_train_csv': 'TRAINING_CSV_FILE_NAME',\n",
       " 'model_sentiment': 'gpt-3.5-turbo-1106',\n",
       " 'hyperparams': {'learning_rate_multiplier': 0.5,\n",
       "  'batch_size': 125,\n",
       "  'n_epochs': 6},\n",
       " 'model_benchwork': 'gpt-3.5-turbo-0125',\n",
       " 'model_embed': 'text-embedding-3-small',\n",
       " 'n_bs': 1000,\n",
       " 'THRES_NUM_PAIR_COLLAB': 100,\n",
       " 'dist_max': 6,\n",
       " 'year_ranges': [[-4, -3], [-2, -1], [0, 0], [1, 2], [3, 4], [5, 6]],\n",
       " 'binW': 30}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dir_input, \"params.json\")) as f:\n",
    "    params = json.load(f)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download select xml files using oa_file_list.csv (in \"input\" folder) <br />\n",
    "which is downloaded from https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below row's 2nd col doesn't follow expected formatting behavior:\n",
      "['oa_package/c2/56/PMC5871948.tar.gz', 'Diseases. 02017 Dec 22; 6(1):2']\n",
      "below row's 2nd col doesn't follow expected formatting behavior:\n",
      "['oa_package/c2/56/PMC5871948.tar.gz', 'Diseases. 02017 Dec 22; 6(1):2']\n",
      "below row's 2nd col doesn't follow expected formatting behavior:\n",
      "['oa_package/0c/83/PMC5977326.tar.gz', 'Nanomaterials (Basel). 02018 May 9; 8(5):312']\n",
      "below row's 2nd col doesn't follow expected formatting behavior:\n",
      "['oa_package/0c/83/PMC5977326.tar.gz', 'Nanomaterials (Basel). 02018 May 9; 8(5):312']\n"
     ]
    }
   ],
   "source": [
    "import filter_papers as filt_pap\n",
    "\n",
    "# Save journal meta info, such as name and JIF.\n",
    "filt_pap.save_jour2meta(dir_input, dir_dict, \"JCR_JournalResults_05_2024\", jif_thres=params[\"jif_thres\"])\n",
    "\n",
    "# Filter by pub year and journals.\n",
    "founD = loadPKL(dir_dict, \"jour2meta\")\n",
    "jrns = set()  # 187 journals (total 188, but 1 is absent in PMC).\n",
    "for k, v in founD.items():\n",
    "    MedAbbr = v[\"MedAbbr\"]\n",
    "    assert MedAbbr not in jrns, f\"Duplicate MedAbbr found for journal={MedAbbr}.\"\n",
    "    jrns.add(MedAbbr)\n",
    "\n",
    "# stats_dict, stats_dict_filtered are saved in /TEMP; for descriptive stats.\n",
    "# oa_file_list_filtered.csv is saved in /TEMP too; specifies what papers we will download next.\n",
    "stats_dict, stats_dict_filtered = filt_pap.filter_file_list(params[\"year_range\"], jrns, dir_input, dir_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_xml import download_tgz_files\n",
    "\n",
    "# We have oa_file_list_filtered.csv, now we download xml from it.\n",
    "# Run below to download xml files we need from PubMedCentral OpenAccess Subset.\n",
    "download_tgz_files(csv_in=dir_TEMP, xml_out=dir_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have xml files, we now process them and do some filtering <br/>\n",
    "we use xml to extract following data:\n",
    "1. citations\n",
    "2. citation sentences\n",
    "3. first, last, all author first and last names\n",
    "4. affiliations -> we will extract department and country from this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download punkt for tokenization.\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download nltk punkt for sentence tokenization.\n",
    "nltk.download(\"punkt\")  # nltk 3.8.1 just needs this one.\n",
    "# nltk.download('punkt_tab')  # In nltk 3.9.1 this is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml_parser as xp\n",
    "\n",
    "with open(os.path.join(dir_TEMP, \"stats_dict_filtered.pkl\"), \"rb\") as f:\n",
    "    stats_dict_filtered = pickle.load(f)\n",
    "# Parse the downloaded xml files; save key_info_all.pkl in /TEMP.\n",
    "key_info_all = xp.parse_all_xml_files(dir_xml, dir_TEMP, stats_dict_filtered[\"journal_year_lookup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125342/125342 [00:13<00:00, 9229.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract info from key_info_all.pkl; save ref_stats.pkl in /TEMP.\n",
    "ref_stats = xp.make_ref_stats(dir_TEMP, key_info_all, stats_dict_filtered[\"journal_year_lookup\"], jrns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125342/125342 [00:07<00:00, 16229.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import get_meta_data as gmd\n",
    "\n",
    "# Make citation edges and article_meta and save them to /TEMP.\n",
    "gmd.make_edges_and_meta(dir_TEMP, ref_stats, key_info_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here on we don't need key_info_all or ref_stats,\n",
    "### instead we primarily use article_meta and paper2meta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get some data from external sources, using various APIs:\n",
    "1. sentiment scores (ChatGPT):\n",
    "    * sentences and info files in /TEMP\n",
    "2. benchwork score (ChatGPT)\n",
    "    * we need paper content (starting from introduction, may also cover some of results or method sections) to get this one; paper content in /TEMP\n",
    "3. h-Index (WoS) & gender (gender-API): contains last author name and their h-Index and estimated gender\n",
    "    * last_author2gender-Neuroscience.csv (/input)\n",
    "    * last_author2hIndex.pkl and last_author2gender_info.pkl(/dicts)\n",
    "    * Need author names for this one.\n",
    "4. power distance and individualism (https://geerthofstede.com/research-and-vsm/dimension-data-matrix/): 6-dimensions-for-website-2015-08-16.csv (/input)\n",
    "\n",
    "5. brilliance (https://doi.org/10.1037/edu0000669): brilliance_data.csv (/input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import external_methods as em\n",
    "import process_field_and_country as pfc\n",
    "import cite_coauthor_functions as ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weakly connected components stats:\n",
      "total: 113535 articles\n",
      "largest component ('subset'): 111577 articles\n",
      "remaining 845 components:\n",
      "\t1958 articles\n",
      "\tmean: 2.32 articles\n",
      "\tstd: 0.78 articles\n",
      "\tmedian: 2.0 articles\n"
     ]
    }
   ],
   "source": [
    "# Create two files saved in /TEMP:\n",
    "# \"sentences2rate\" txt file to be used for ChatGPT, \"sentrow2edgeinfo\" pkl file for bookkeeping.\n",
    "em.save_CGPT_input_files(dir_cnets=dir_TEMP, dir_out=dir_TEMP, cite_marker=\"✪\")\n",
    "\n",
    "# Create structured citation data; no sentiment yet; later add sentiment from ChatGPT API results.\n",
    "cite2sent = ccf.make_cite2sent_from_sentence_data(dir_TEMP, dir_TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input()  # Run this cell and then enter your OpenAI api key.\n",
    "file_name = params[\"file_name_train_csv\"]\n",
    "\n",
    "# Finetune ChatGPT model specified in params[\"model_sentiment\"].\n",
    "init_dict = em.CGPT_init(api_key)\n",
    "em.save_finetune_file(init_dict, dir_batch, file_name)\n",
    "em.send_finetune_job(init_dict, params[\"model_sentiment\"], dir_batch, file_name, hyperparams=params[\"hyperparams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate sentence sentiment by creating and uploading batches to OpenAI (ChatGPT).\n",
    "model = \"FINETUNED_MODEL_NUMBER\"  # Copy paste the finetuned model number from OpenAI.\n",
    "em.creat_batch_jobs(api_key, model, dir_TEMP, dir_batch, batch_size=49999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From OpenAI, download output files (jsonl format) to /batch folder. <br/>\n",
    "We will process these jsonl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate cite2sent with empirical/observed/measured sentiment from ChatGPT output in dir_batch.\n",
    "ccf.update_cite2sent_from_row2rate(dir_TEMP, dir_batch)\n",
    "# Apply hierarchy rule such that each pair of papers only has at most 1 sentiment.\n",
    "# We also make \"cite2ns\" dict, which contains number of citation sentencees for each citation pair.\n",
    "ccf.update_cite2sent_hierarchy_rule(dir_TEMP, dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, the scope of papers is those in cite2sent (made by ccf module). <br />\n",
    "All data and look up dictionaries (e.g., paper2authors) for figures are in /dicts. <br />\n",
    "Below, we extract relevant metadata from paper2meta and turn them into individual lookup dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_meta_data\n",
    "\n",
    "paper2meta = ccf.make_paper2meta(list(cite2sent.keys()), dir_TEMP, dir_dict)\n",
    "get_meta_data.save_paper_author_dicts(paper2meta, dir_dict)\n",
    "get_meta_data.save_paper_time_dicts(paper2meta, dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper title parsing using a separate parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108909/108909 [07:34<00:00, 239.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# If it doesn't print that titles not found, then all titles found, proceed to next stage.\n",
    "get_meta_data.save_and_parse_full_titles(dir_xml, dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title embedding from OpenAI (ETA: 8 hours), save to /dicts.\n",
    "api_key = input()  # Run this cell and then enter your OpenAI api key.\n",
    "em.get_title_embedding(dir_dict, dir_TEMP, api_key, model=params[\"model_embed\"])\n",
    "em.save_title_embedding(dir_dict, dir_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given title embedding, we calculate title similarity for each citation pair; \"cite2title_sim\" dict.\n",
    "ccf.save_cite2title_sim(dir_batch, list(cite2sent.keys()), dir_TEMP)\n",
    "\n",
    "# Create coauthorship network to calculate social distance (collaboration distance AKA CD).\n",
    "ccf.save_g_coau_t(dir_dict)\n",
    "ccf.save_cite2distance(list(cite2sent.keys()), dir_dict)\n",
    "\n",
    "# Need 4 files: cite2ns, cite2title_sim in /TEMP; cite2sent_2, paper2meta in /dicts.\n",
    "ccf.save_cite2sent_null_param(dir_dict, dir_TEMP, maxN=15, n_min_samp=500)\n",
    "# Use g_coau_t and metadata to make a dict mapping citation pairs to time before first collab.\n",
    "ccf.save_cite2t_collab(dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find last author's departments and countries for each paper.\n",
    "pfc.save_department_dicts(paper2meta, os.path.join(dir_input, \"department_names.csv\"), dir_dict, print_fail=False)\n",
    "pfc.save_country_dicts(paper2meta, dir_dict, print_fail=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Benchwork Score & Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save .txt file sent to ChatGPT to rate benchwork score, 100 for each of 28 departments.\n",
    "em.save_paper_snippet(dir_xml, dir_dict, dir_TEMP, n_paper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to send to ChatGPT to rate benchwork score.\n",
    "\n",
    "api_key = input()\n",
    "init_dict = em.CGPT_init_benchwork(api_key)\n",
    "with open(os.path.join(dir_TEMP, \"benchwork_text_CGPT.txt\"), mode=\"r+\", encoding=\"UTF-8\") as file_out:\n",
    "    txt_to_send = file_out.readlines()\n",
    "with open(os.path.join(dir_TEMP, \"benchwork_text_row2paper.pkl\"), \"rb\") as f:\n",
    "    benchwork_text_row2paper = pickle.load(f)\n",
    "\n",
    "benchwork_text_row2response = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to ChatGPT to rate benchwork score; ETA: 2 hours.\n",
    "for row, txt in enumerate(txt_to_send):\n",
    "    if row not in benchwork_text_row2response:\n",
    "        time.sleep(0.05)  # Prevent from hitting rate limit; in seconds.\n",
    "        try:\n",
    "            res = em.get_rating(txt, init_dict)\n",
    "            # Print irregular/unexpected response.\n",
    "            if \"no\" not in res.casefold() and \"yes\" not in res.casefold():\n",
    "                print(f\"row={row} PMC={benchwork_text_row2paper[row]}, GPT response: {res}\")\n",
    "            benchwork_text_row2response[row] = res\n",
    "        except:\n",
    "            print(f\"Error encountered at row={row} PMC={benchwork_text_row2paper[row]}\")\n",
    "    if row % 50 == 0 or row == (len(txt_to_send) - 1):  # Save periodically.\n",
    "        with open(os.path.join(dir_batch, \"benchwork_text_row2response.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(benchwork_text_row2response, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row=733 PMC=6518697, GPT response: ----> No <----, different from previous response to the same paper; this paper discarded.\n",
      "row=2715 PMC=3682120, GPT response: ----> No <----, different from previous response to the same paper; this paper discarded.\n"
     ]
    }
   ],
   "source": [
    "# Process ChatGPT response, save to dictionary for department-wise measures later.\n",
    "pfc.save_benchwork_count(dir_TEMP, dir_batch, dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. h-Index and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get h-Index. second_try uses different data to attempt to get h-Index.\n",
    "WOS_api_key = input()  # Run this cell and then enter your WOS api key.\n",
    "em.get_author_meta_from_WOS(dir_dict, WOS_api_key, second_try=False, verbose=False)\n",
    "em.save_last_author2hIndex(dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of names in .csv and manually send it to gender-api.com.\n",
    "# Also use it to for Web of Science API to retrieve author metadata, in which we obtain h-Index.\n",
    "# This method is preferred because it is orders of magnitude faster (only need minutes).\n",
    "em.prepare_author_names(dir_dict, dir_TEMP)\n",
    "\n",
    "# Process gender.\n",
    "# Make sure to download the output file from gender-api.com and save it in /input folder, naming it gender-API.csv.\n",
    "# Process gender-API.csv (in /input) and create last author to gender info lookup dict.\n",
    "em.save_author_gender(dir_dict, dir_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. and 5. Save Department and Country measures.\n",
    "\n",
    "Department: benchwork, synthesis, brilliance, proportion of men <br/>\n",
    "Country: Power Distance, individualism, proportion of men <br/>\n",
    "\n",
    "The department/country has to have at least 50 post-hierarchy citations towards collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 departments have 100+ post-hierarchy citations towards collaborators.\n",
      "23 countries have 100+ post-hierarchy citations towards collaborators.\n"
     ]
    }
   ],
   "source": [
    "# Brilliance data should be stored in dir_input (/input), named \"brilliance_data.csv\".\n",
    "pfc.save_department_measures(dir_input, dir_dict, thres=params[\"THRES_NUM_PAIR_COLLAB\"])\n",
    "pfc.save_country_measures(dir_input, dir_dict, thres=params[\"THRES_NUM_PAIR_COLLAB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: Prepare data to plot <br/>\n",
    "This will take 11 hours because it involves random sampling that's different at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_plot_data as ppd\n",
    "\n",
    "\n",
    "ppd.prepare_collab_groups(dir_dict, dir_npy, n_bs=params[\"n_bs\"])  # 2 hours.\n",
    "ppd.prepare_collab_distance(dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"])  # 0.5 hours.\n",
    "ppd.prepare_t_collab(dir_dict, dir_npy, year_ranges=params[\"year_ranges\"], n_bs=params[\"n_bs\"])  # 1 hour.\n",
    "ppd.prepare_hindex(dir_dict, dir_npy, binW=params[\"binW\"], n_bs=params[\"n_bs\"])  # 1.3 hours.\n",
    "ppd.prepare_country_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours.\n",
    "ppd.prepare_department_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 3.5 hours.\n",
    "ppd.prepare_gender_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have all ingredients we need for figures. Run plot_ ipynb to make figures. <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
