{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load parameters json file in /input.\n",
    "\n",
    "\n",
    "THRES_NUM_PAIR_COLLAB (int):</br>\n",
    "Sample size threshold for at least this amount of citation pairs to collaborators.</br>\n",
    "Used to filter out departments and countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import external_methods as em\n",
    "import process_field_and_country as pfc\n",
    "import cite_coauthor_functions as ccf\n",
    "from helper_functions import loadPKL, savePKL, get_IRR, pval_star\n",
    "\n",
    "CWD = os.path.abspath(\"\")  # Jupyter notebook path.\n",
    "\n",
    "dir_input = os.path.join(CWD, \"input\")\n",
    "dir_batch = os.path.join(CWD, \"batch\")  # ChatGPT related output.\n",
    "dir_TEMP = os.path.join(CWD, \"TEMP\")  # Intermediate files.\n",
    "dir_dict = os.path.join(CWD, \"dicts\")  # Look up dictionaries such as paper2meta; main data directory.\n",
    "dir_npy = os.path.join(CWD, \"npy\")  # Data files needed for plotting figures.\n",
    "dir_output = os.path.join(CWD, \"output\")  # Figures.\n",
    "dir_xml = os.path.join(CWD, \"xml\")  # xml files.\n",
    "dir_DEBUG = os.path.join(CWD, \"DEBUG\")\n",
    "\n",
    "with open(os.path.join(dir_input, \"params.json\")) as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "api_key = input()  # Run this cell and then enter your OpenAI api key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download select xml files using oa_file_list.csv (in \"input\" folder) <br />\n",
    "which is downloaded from https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filter_papers as filt_pap\n",
    "\n",
    "# Save journal meta info, such as name and JIF.\n",
    "filt_pap.save_jour2meta(dir_input, dir_dict, \"JCR_JournalResults_05_2024\", jif_thres=params[\"jif_thres\"])\n",
    "\n",
    "# Filter by pub year and journals.\n",
    "founD = loadPKL(dir_dict, \"jour2meta\")\n",
    "jrns = set()  # 187 journals (total 188, but 1 is absent in PMC).\n",
    "for k, v in founD.items():\n",
    "    MedAbbr = v[\"MedAbbr\"]\n",
    "    assert MedAbbr not in jrns, f\"Duplicate MedAbbr found for journal={MedAbbr}.\"\n",
    "    jrns.add(MedAbbr)\n",
    "\n",
    "# stats_dict, stats_dict_filtered are saved in /TEMP; for descriptive stats.\n",
    "# oa_file_list_filtered.csv is saved in /TEMP too; specifies what papers we will download next.\n",
    "stats_dict, stats_dict_filtered = filt_pap.filter_file_list(params[\"year_range\"], jrns, dir_input, dir_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_xml import download_tgz_files\n",
    "\n",
    "# We have oa_file_list_filtered.csv, now we download xml from it.\n",
    "# Run below to download xml files we need from PubMedCentral OpenAccess Subset.\n",
    "download_tgz_files(csv_in=dir_TEMP, xml_out=dir_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have xml files, we now process them and do some filtering <br/>\n",
    "we use xml to extract following data:\n",
    "1. citations\n",
    "2. citation sentences\n",
    "3. first, last, all author first and last names\n",
    "4. affiliations -> we will extract department and country from this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download punkt for tokenization.\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download nltk punkt for sentence tokenization.\n",
    "nltk.download(\"punkt\")  # nltk 3.8.1 just needs this one.\n",
    "# nltk.download('punkt_tab')  # In nltk 3.9.1 this is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml_parser as xp\n",
    "\n",
    "with open(os.path.join(dir_TEMP, \"stats_dict_filtered.pkl\"), \"rb\") as f:\n",
    "    stats_dict_filtered = pickle.load(f)\n",
    "# Parse the downloaded xml files; save key_info_all.pkl in /TEMP.\n",
    "key_info_all = xp.parse_all_xml_files(dir_xml, dir_TEMP, stats_dict_filtered[\"journal_year_lookup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract info from key_info_all.pkl; save ref_stats.pkl in /TEMP.\n",
    "ref_stats = xp.make_ref_stats(dir_TEMP, key_info_all, stats_dict_filtered[\"journal_year_lookup\"], jrns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_meta_data as gmd\n",
    "\n",
    "# Make citation edges and article_meta and save them to /TEMP.\n",
    "gmd.make_edges_and_meta(dir_TEMP, ref_stats, key_info_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here on we don't need key_info_all or ref_stats,\n",
    "### instead we primarily use article_meta and paper2meta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get some data from external sources, using various APIs:\n",
    "1. sentiment scores (ChatGPT):\n",
    "    * sentences and info files in /TEMP\n",
    "2. benchwork score (ChatGPT)\n",
    "    * we need paper content (starting from introduction, may also cover some of results or method sections) to get this one; paper content in /TEMP\n",
    "3. h-Index (WoS) & gender (gender-API): contains last author name and their h-Index and estimated gender\n",
    "    * last_author2gender-Neuroscience.csv (/input)\n",
    "    * last_author2hIndex.pkl and last_author2gender_info.pkl(/dicts)\n",
    "    * Need author names for this one.\n",
    "4. power distance and individualism (https://geerthofstede.com/research-and-vsm/dimension-data-matrix/): 6-dimensions-for-website-2015-08-16.csv (/input)\n",
    "\n",
    "5. brilliance (https://doi.org/10.1037/edu0000669): brilliance_data.csv (/input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two files saved in /TEMP:\n",
    "# \"sentences2rate\" txt file contains all sentences to be rated by ChatGPT, \"sentrow2edgeinfo\" pkl file for bookkeeping.\n",
    "em.save_CGPT_input_files(dir_cnets=dir_TEMP, dir_out=dir_TEMP, cite_marker=\"âœª\")\n",
    "\n",
    "# Create structured citation data; no sentiment yet; later add sentiment from ChatGPT API results.\n",
    "cite2sent = ccf.make_cite2sent_from_sentence_data(dir_TEMP, dir_TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, the scope of papers is those in cite2sent (made by ccf module). <br />\n",
    "All data and look up dictionaries (e.g., paper2authors) for figures are in /dicts. <br />\n",
    "Below, we extract relevant metadata from paper2meta and turn them into individual lookup dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_meta_data\n",
    "\n",
    "paper2meta = ccf.make_paper2meta(list(cite2sent.keys()), dir_TEMP, dir_dict)\n",
    "get_meta_data.save_paper_author_dicts(paper2meta, dir_dict)\n",
    "get_meta_data.save_paper_time_dicts(paper2meta, dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before continuing, now the full sentence and paper2meta datasets are ready, we will do human raters codebook rating and ChatGPT model validation on sentence sentiment and paper benchwork.\n",
    "Open `chatgpt_validation.ipynb` for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sentiment\n",
    "\n",
    "cost about $150 with CGPT_init_crit_5_explain() and 627108 sentences, gpt-4.1-mini (input $0.4/1M tokens, output $1.6/1M tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = params[\"file_name_train_csv\"]\n",
    "\n",
    "# # Finetune ChatGPT model specified in params[\"model_sentiment\"].\n",
    "# init_dict = em.CGPT_init(api_key)\n",
    "# em.save_finetune_file(init_dict, dir_batch, file_name)\n",
    "# em.send_finetune_job(init_dict, params[\"model_sentiment\"], dir_batch, file_name, hyperparams=params[\"hyperparams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_path_out = os.path.join(dir_batch, \"sentiment_n=627K\")\n",
    "fname = \"sentences2rate-CGPT\"\n",
    "init_dict = em.CGPT_init_crit_5_explain(api_key)\n",
    "print(params[\"model_sentiment\"])\n",
    "batch_size = 45000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 5\n",
    "# 0,1-4(1-3 are done by nonbatch, 4 there's 2593 done by nonbatch),8-13\n",
    "subbatch = 5000\n",
    "\n",
    "# tier-1\n",
    "# \"gpt-3.5-turbo-1106\" 200,000/250=800 (500 RPM), 0.12 interval, 10K RPD.\n",
    "# \"gpt-4.1-mini-2025-04-14\" 200,000/250=800 (500 RPM), 0.12 interval.\n",
    "# \"gpt-4.1-2025-04-14\" 30,000/250=120 RPM, 0.5 interval. nonbatch cost $4.96 (8755 items).\n",
    "\n",
    "# tier-3\n",
    "# \"gpt-3.5-turbo\" 5K RPM, 0.012 interval, 40M TPD.\n",
    "# \"gpt-4.1-mini-2025-04-14\" 5K RPM, 0.012 interval, 40M TPD.\n",
    "# \"gpt-4.1-2025-04-14\" 5K RPM, 0.012 interval. 100M TPD.\n",
    "\n",
    "# ~700(input)+100(output) tokens per request. 627K sentences/requests. = 500M+\n",
    "# Do it in 14 batches, 44800 sentences per batch...\n",
    "em.creat_batch_jobs_fc(init_dict, params[\"model_sentiment\"], dir_TEMP, batch_path_out, batch_size=batch_size, fname=fname, batch_num=batch_num, bench=False, token_output=100, subbatch=subbatch)\n",
    "# em.send_request_nonbatch_fc(init_dict, params[\"model_sentiment\"], dir_TEMP, batch_path_out, fname=fname, i=None, lab=None, interval=0.012, num=100000, bench=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because batch is unpredictable and thus slow overall, so run nonbatch at the same time to speed up the process. See generate_data_only_nonbatch_run.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From OpenAI, download output files (jsonl format) to /batch folder. <br/>\n",
    "We will process these jsonl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download batch output files programmatically.\n",
    "batches_dict = loadPKL(batch_path_out, \"gpt-4.1-mini-2025-04-14-batches_dict\")\n",
    "em.download_batch_output(api_key, batches_dict, batch_path_out, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batch output files to get raw row2rate and processed row2rate.\n",
    "em.process_batch_outputs_fc(batch_path_out, batch_path_out, i=\"batch\", lab=None)\n",
    "row2rate = em.process_row2rate(loadPKL(batch_path_out, f\"gpt-4.1-mini-2025-04-14-row2rate_reason-batch\"), verbose=False, fc=True, bench=False)\n",
    "print(Counter([v[0] for v in row2rate.values()]))\n",
    "\n",
    "row2rate_nonba = em.process_row2rate(loadPKL(batch_path_out, f\"gpt-4.1-mini-2025-04-14-row2rate_reason-1to5\"), verbose=False, fc=True, bench=False)\n",
    "print(Counter([v[0] for v in row2rate_nonba.values()]))\n",
    "\n",
    "# Combine batch and non-batch results, and save it to dir_batch.\n",
    "row2rate.update(row2rate_nonba)\n",
    "print(Counter([v[0] for v in row2rate.values()]))\n",
    "print(len(row2rate), max(row2rate.keys())-min(row2rate.keys())+1, sep=\"\\n\")\n",
    "savePKL(dir_batch, \"row2rate_reason_processed\", row2rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate cite2sent with empirical/observed/measured sentiment from ChatGPT output in dir_batch.\n",
    "ccf.update_cite2sent_from_row2rate(dir_TEMP, dir_batch)  # cite2sent_0 -> 1\n",
    "# Apply hierarchy rule such that each pair of papers only has at most 1 sentiment.\n",
    "# We also make \"cite2ns\" dict, which contains number of citation sentencees for each citation pair.\n",
    "ccf.update_cite2sent_hierarchy_rule(dir_TEMP, dir_dict)  # cite2sent_1 -> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper title parsing using a separate parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it doesn't print that titles not found, then all titles found, proceed to next stage.\n",
    "get_meta_data.save_and_parse_full_titles(dir_xml, dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title embedding from OpenAI (ETA: 8 hours), save to /dicts.\n",
    "api_key = input()  # Run this cell and then enter your OpenAI api key.\n",
    "em.get_title_embedding(dir_dict, dir_TEMP, api_key, model=params[\"model_embed\"])\n",
    "em.save_title_embedding(dir_dict, dir_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite2sent = loadPKL(dir_TEMP, \"cite2sent_0\")\n",
    "# Given title embedding, we calculate title similarity for each citation pair; \"cite2title_sim\" dict.\n",
    "ccf.save_cite2title_sim(dir_batch, list(cite2sent.keys()), dir_TEMP)\n",
    "\n",
    "# Create coauthorship network to calculate social distance (collaboration distance AKA CD).\n",
    "ccf.save_g_coau_t2(dir_dict, weight_nocollab=np.inf, weight_type=\"binary\", no_mid_mid=False, lag=0)\n",
    "ccf.save_cite2distance(list(cite2sent.keys()), dir_dict)\n",
    "\n",
    "# Need 4 files: cite2ns, cite2title_sim in /TEMP; cite2sent_2, paper2meta in /dicts.\n",
    "ccf.save_cite2sent_null_param(dir_dict, dir_TEMP, maxN=15, n_min_samp=500)\n",
    "# Use g_coau_t and metadata to make a dict mapping citation pairs to time before first collab.\n",
    "ccf.save_cite2t_collab(dir_dict, lag=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper2meta = loadPKL(dir_dict, \"paper2meta\")\n",
    "# Find last author's departments and countries for each paper.\n",
    "pfc.save_department_dicts(paper2meta, os.path.join(dir_input, \"department_names.csv\"), dir_dict, print_fail=False)\n",
    "pfc.save_country_dicts(paper2meta, dir_dict, print_fail=False)\n",
    "\n",
    "# Department cultural capital (international and interdisciplinary collab measures).\n",
    "pfc.save_country_dicts_all_authors(paper2meta, dir_dict, print_fail=False)\n",
    "pfc.save_department_dicts_all_authors(paper2meta, os.path.join(dir_input, \"department_names.csv\"), dir_dict, print_fail=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Benchwork Score & Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save .txt file sent to ChatGPT to rate benchwork score, 100 for each of 28 departments.\n",
    "em.save_paper_snippet(dir_xml, dir_dict, dir_TEMP, n_paper=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch num_tokens=25,674,366\n",
      "1 batches created.\n"
     ]
    }
   ],
   "source": [
    "# ~9K(input) tokens per request. (With 100K context window.) 2800 requests.\n",
    "# $5 for 4.1-mini: Input $0.40/1M tokens, Cached input $0.10/1M tokens, Output $1.60/1M tokens.\n",
    "fname = f\"benchwork_text_CGPT\"\n",
    "init_dict = em.CGPT_init_benchwork(api_key)\n",
    "em.creat_batch_jobs(init_dict, params[\"model_benchwork\"], dir_TEMP, dir_batch, batch_size=3000, fname=fname, batch_num=None)\n",
    "# em.send_request_nonbatch(init_dict, model, dir_TEMP, dir_batch, fname=fname, i=ib, lab=lab, interval=0.012, num=1000, bench=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.process_batch_outputs(dir_batch, dir_batch)\n",
    "row2rate0 = em.process_row2rate(loadPKL(dir_batch, f\"gpt-4.1-mini-2025-04-14-row2rate\"), verbose=False, fc=False, bench=True)\n",
    "Counter([v for v in row2rate0.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to dir_batch \"bench_n=2800\", manually rename \"gpt-4.1-mini-2025-04-14-row2rate.pkl\" to \"benchwork_text_row2response.pkl\", and put in dir_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ChatGPT response, save to dictionary for department-wise measures later.\n",
    "pfc.save_benchwork_count(dir_TEMP, dir_batch, dir_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. h-Index and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get h-Index. second_try uses different data to attempt to get h-Index.\n",
    "WOS_api_key = input()  # Run this cell and then enter your WOS api key.\n",
    "em.get_author_meta_from_WOS(dir_dict, WOS_api_key, second_try=False, verbose=False)\n",
    "em.save_last_author2hIndex(dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of names in .csv and manually send it to gender-api.com.\n",
    "# Also use it to for Web of Science API to retrieve author metadata, in which we obtain h-Index.\n",
    "# This method is preferred because it is orders of magnitude faster (only need minutes).\n",
    "em.prepare_author_names(dir_dict, dir_TEMP)\n",
    "\n",
    "# Process gender.\n",
    "# Make sure to download the output file from gender-api.com and save it in /input folder, naming it gender-API.csv.\n",
    "# Process gender-API.csv (in /input) and create last author to gender info lookup dict.\n",
    "em.save_author_gender(dir_dict, dir_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. and 5. Save Department and Country measures.\n",
    "\n",
    "Department: benchwork, synthesis, brilliance, proportion of men <br/>\n",
    "Country: Power Distance, individualism, proportion of men <br/>\n",
    "\n",
    "The department/country has to have at least 100 post-hierarchy citations towards collaborators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brilliance data should be stored in dir_input (/input), named \"brilliance_data.csv\".\n",
    "pfc.save_department_measures(dir_input, dir_dict, thres=params[\"THRES_NUM_PAIR_COLLAB\"])\n",
    "pfc.save_country_measures(dir_input, dir_dict, thres=params[\"THRES_NUM_PAIR_COLLAB\"])\n",
    "pfc.save_department_collab_measures(dir_dict, dir_dict, thres=params[\"THRES_NUM_PAIR_COLLAB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: Prepare data to plot <br/>\n",
    "This will take 11 hours because it involves random sampling that's different at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_plot_data as ppd\n",
    "\n",
    "ppd.prepare_collab_groups(dir_dict, dir_npy, n_bs=params[\"n_bs\"])  # 2 hours.\n",
    "ppd.prepare_collab_distance(dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"])  # 0.5 hours.\n",
    "ppd.prepare_department_effects_collab(\n",
    "    dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"]\n",
    ")  # 1.5 hours.\n",
    "ppd.prepare_t_collab(dir_dict, dir_npy, year_ranges=params[\"year_ranges\"], n_bs=params[\"n_bs\"])  # 1 hour.\n",
    "ppd.prepare_hindex(dir_dict, dir_npy, binW=params[\"binW\"], n_bs=params[\"n_bs\"])  # 1.3 hours.\n",
    "ppd.prepare_country_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours.\n",
    "ppd.prepare_department_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 3.5 hours.\n",
    "ppd.prepare_gender_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have all ingredients we need for figures. Run plot_ ipynb to make figures. <br/>\n",
    "\n",
    "### below chuncks are supplementary analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is for multiple linear regression (moderator analysis) across multiple groups, so null model and y-measure is on citation level at pre-aggregation, which is different from the vanilla group of citation level at post-aggregation\n",
    "\n",
    "### create null for pre-agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In bins whose sample size < 500\n",
      "there are 21 bins, 6050.0 citation pairs,\n",
      "taking up 0.91% of all pairs.\n"
     ]
    }
   ],
   "source": [
    "# Find null.\n",
    "ccf.save_cite2sent_null_param_pre_agg(dir_dict, dir_TEMP, maxN=15, n_min_samp=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save y (mean and std for each sentiment) for each cite pair.\n",
    "cite2sent_1 = loadPKL(dir_TEMP, \"cite2sent_1\")\n",
    "cite2sent_n = loadPKL(dir_dict, \"cite2sent_null_param_pre_agg\")\n",
    "ccf.find_y_rand_samp_pre_agg(cite2sent_1, cite2sent_n, n_rand_samp=params[\"n_bs\"], full_samp=False, save_as_cite_dict=dir_dict)  # Save as cite2SRPC_ms.pkl, to be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import save_Rds\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Build table of citation pairs for multiple linear regression in R.\n",
    "department2synthesis = loadPKL(dir_dict, \"department2synthesis\")\n",
    "department2benchwork = loadPKL(dir_dict, \"department2benchwork\")\n",
    "df = pd.DataFrame.from_dict(dict(synthesis=department2synthesis, benchwork=department2benchwork))\n",
    "save_Rds(dir_dict, \"department2meta\", df)\n",
    "\n",
    "country2power_distance = loadPKL(dir_dict, \"country2power_distance\")\n",
    "country2individualism = loadPKL(dir_dict, \"country2individualism\")\n",
    "df = pd.DataFrame.from_dict(dict(power_distance=country2power_distance, individualism=country2individualism))\n",
    "save_Rds(dir_dict, \"country2meta\", df)\n",
    "\n",
    "# Paper meta data.\n",
    "paper2last_author_department = loadPKL(dir_dict, \"paper2last_author_department_28_dep\")\n",
    "paper2last_author = loadPKL(dir_dict, \"paper2last_author\")  # Last author name of each paper.\n",
    "last_author2gender_info = loadPKL(dir_dict, \"last_author2gender_info\")\n",
    "last_author2hIndex = loadPKL(dir_dict, \"last_author2hIndex\")  # Last author hIndex.\n",
    "paper2last_author_gender = {p: last_author2gender_info[au] for p, au in paper2last_author.items()}\n",
    "paper2last_author_gender = {p: g[0] if (g[1] >= 0.7 and g[2] >= 20) else None for p, g in paper2last_author_gender.items()}\n",
    "cite2SRPC_ms = loadPKL(dir_dict, \"cite2SRPC_ms\")\n",
    "\n",
    "# Citation table.\n",
    "tmp_data_dict = dict()\n",
    "tmp_data_dict[\"cite_pair\"] = {i: str(c) for i, c in enumerate(cite2SRPC_ms)}\n",
    "tmp_data_dict[\"y_pos\"] = {i: ms[0][0] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"y_neu\"] = {i: ms[0][1] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"y_neg\"] = {i: ms[0][2] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"dy_pos\"] = {i: ms[1][0] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"dy_neu\"] = {i: ms[1][1] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"dy_neg\"] = {i: ms[1][2] for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "# Inverse variance weights. np.nan if std is 0.\n",
    "tmp_data_dict[\"wy_pos\"] = {i: 1 / ms[1][0] ** 2 if ms[1][0] != 0 else np.nan for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"wy_neu\"] = {i: 1 / ms[1][1] ** 2 if ms[1][1] != 0 else np.nan for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"wy_neg\"] = {i: 1 / ms[1][2] ** 2 if ms[1][2] != 0 else np.nan for i, ms in enumerate(cite2SRPC_ms.values())}\n",
    "tmp_data_dict[\"last_author_department\"] = {i: paper2last_author_department[c[0]] for i, c in enumerate(cite2SRPC_ms)}\n",
    "tmp_data_dict[\"citing_gender\"] = {i: paper2last_author_gender.get(c[0], \"\") for i, c in enumerate(cite2SRPC_ms)}\n",
    "tmp_data_dict[\"cited_gender\"] = {i: paper2last_author_gender.get(c[1], \"\") for i, c in enumerate(cite2SRPC_ms)}\n",
    "\n",
    "\n",
    "def _nonce_get_hindex_diff(pi, pj, abs_=False):\n",
    "    hi = last_author2hIndex.get(paper2last_author[pi], None)\n",
    "    if hi is None:\n",
    "        return None\n",
    "    hj = last_author2hIndex.get(paper2last_author[pj], None)\n",
    "    if hj is None:\n",
    "        return None\n",
    "    return np.abs(hi - hj) if abs_ else hi - hj\n",
    "\n",
    "\n",
    "tmp_data_dict[\"hindex_diff\"] = {i: _nonce_get_hindex_diff(c[0], c[1], abs_=False) for i, c in enumerate(cite2SRPC_ms)}\n",
    "tmp_data_dict[\"hindex_abs\"] = {i: _nonce_get_hindex_diff(c[0], c[1], abs_=True) for i, c in enumerate(cite2SRPC_ms)}\n",
    "df = pd.DataFrame.from_dict(tmp_data_dict)\n",
    "# Both filled and empty lists will act fine with json.dumps; the NaN ones will become empty list first.\n",
    "df[\"last_author_department\"] = df[\"last_author_department\"].apply(lambda x: json.dumps(x) if isinstance(x, list) else json.dumps([]))\n",
    "save_Rds(dir_dict, \"cite2author_aff\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is for sensitivity of num of sentences on sentiment.\n",
    "\n",
    "### create null for ns=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_plot_data as ppd\n",
    "\n",
    "maxN = 10\n",
    "\n",
    "ccf.save_cite2sent_null_param(dir_dict, dir_TEMP, maxN=maxN, n_min_samp=500)\n",
    "print(f\"null param ns-cutoff={maxN} done.\")\n",
    "ppd.prepare_collab_distance(dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"], maxN=maxN)  # 0.5 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below is for synthesis result without review/research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf.save_cite2sent_null_param_only(dir_dict, dir_TEMP, maxN=15, n_min_samp=500, which_only=\"research-article\")\n",
    "ppd.prepare_department_effects_only(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"], which_only=\"research-article\")  # 2.6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf.save_cite2sent_null_param_only(dir_dict, dir_TEMP, maxN=15, n_min_samp=500, which_only=\"review-article\")\n",
    "ppd.prepare_department_effects_only(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"], which_only=\"review-article\")  # 2.6 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emergency debugging on review effects\n",
    "\n",
    "ppdnr is now slightly outdated because of the fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_plot_data_no_review as ppdnr\n",
    "\n",
    "ppdnr.prepare_collab_groups(dir_dict, dir_npy, n_bs=params[\"n_bs\"])  # 2 hours.\n",
    "ppdnr.prepare_collab_distance(dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"])  # 0.5 hours.\n",
    "ppdnr.prepare_department_effects_collab(\n",
    "    dir_dict, dir_npy, dist_max=params[\"dist_max\"], n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"]\n",
    ")  # 1.5 hours.\n",
    "ppdnr.prepare_t_collab(dir_dict, dir_npy, year_ranges=params[\"year_ranges\"], n_bs=params[\"n_bs\"])  # 1 hour.\n",
    "ppdnr.prepare_hindex(dir_dict, dir_npy, binW=params[\"binW\"], n_bs=params[\"n_bs\"])  # 1.3 hours.\n",
    "ppdnr.prepare_country_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours.\n",
    "ppdnr.prepare_department_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 3.5 hours.\n",
    "ppdnr.prepare_gender_effects(dir_dict, dir_npy, n_bs=params[\"n_bs\"], thres=params[\"THRES_NUM_PAIR_COLLAB\"])  # 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
