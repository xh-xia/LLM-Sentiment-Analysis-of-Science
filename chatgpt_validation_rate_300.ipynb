{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import openai\n",
    "import pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "CWD = os.path.abspath(\"\")  # get jupyter notebook path\n",
    "path_ = os.path.join(CWD, \"batch\")\n",
    "file_path_ratings = os.path.join(path_, \"cgpt_ratings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction on Input Files </br>\n",
    "about training/validation csv files </br>\n",
    "- ```rated_training.csv``` and ```rated_validation.csv``` correspond to the full annotator-rated sentences </br>\n",
    "- ```rated_training_more_neg.csv``` contains a subset of samples from ```rated_training.csv``` and ```rated_validation.csv```, that is actually used for fine-tuning (more_neg stands for more critical sentiment sentences); one manually picks rated sentences such that there's a bit more critical in this file, this way the data is more balanced and LLM can see a fair share of what critical sentiment looks like; because otherwise critical sentiment may be too rare to be sufficient for fine-tuning.\n",
    "- ```rated_validation_more_neg.csv``` will be made based on the above 3 files; this will correspond to the holdout dataset that will be used for validation figure making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input()  # Run this cell and then enter your OpenAI api key.\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"For each in-text citation, the rater should measure the sentiment of the citing research toward the cited research (represented as the character âœª), on a scale of -1 to 1. \n",
    "The rater should assign a positive score (+1) to statements depicting the cited research as positive, corroborative, consistent with, similar to, or in common with the citing research.\n",
    "Conversely, the rater should assign a negative score (-1) to statements depicting the cited research as negative, refuting, inconsistent with, dissimilar to, or different from the citing research.\n",
    "If the statements are neutral or do not belong to the aforementioned categories, then the rater should assign 0 to the statements. \n",
    "When you are given a sentence only answer with the numerical results without explanation. \"\"\"\n",
    "\n",
    "\n",
    "user_prompt = \"The sentence to analyze is : \" \n",
    "\n",
    "def get_rating(citation_text):\n",
    "    \n",
    "    messages = [ {\"role\": \"system\", \"content\":  \n",
    "                system_prompt} ] \n",
    "    messages.append( \n",
    "                {\"role\": \"user\", \"content\": user_prompt + citation_text}, \n",
    "            ) \n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model='PUT_MODEL_NUM',  # Put trained model number here.\n",
    "            temperature = 0.01,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)\n",
    "        return None\n",
    "    except openai.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "        print(e)\n",
    "        return None\n",
    "    except openai.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "        return None\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def transform_rating(ratings):\n",
    "    avg = np.mean(ratings)\n",
    "    if avg <= -2 / 5:\n",
    "        return -1\n",
    "    elif avg >= 2 / 5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json(path_):  # path_ points to data folder\n",
    "    f1 = os.path.join(path_, \"rated_training.csv\")\n",
    "    f2 = os.path.join(path_, \"rated_validation.csv\")\n",
    "    # Below have 300 rows, will drop ones in rated_training_more_neg.\n",
    "    df_valid = pd.concat([pd.read_csv(f1, sep=\",\"), pd.read_csv(f2, sep=\",\")], ignore_index=True)\n",
    "    num_ = len(df_valid)\n",
    "    f_train_more_neg = os.path.join(path_, \"rated_training_more_neg.csv\")\n",
    "    df_train = pd.read_csv(f_train_more_neg, sep=\",\")\n",
    "\n",
    "    idxs2drop = []\n",
    "    for index, row in df_train.iterrows():\n",
    "        tmp = df_valid[\"sentences\"] == row[\"sentences\"]\n",
    "        idx = tmp[tmp].index\n",
    "        if len(idx) != 1:\n",
    "            raise Exception(\"Something went wrong #1.\")\n",
    "        idxs2drop.append(idx[0])\n",
    "    df_valid.drop(index=idxs2drop, inplace=True)\n",
    "\n",
    "    if (len(df_train) + len(df_valid)) != num_:\n",
    "        raise Exception(\"Something went wrong #2.\")\n",
    "    df_valid.to_csv(os.path.join(path_, \"rated_validation_more_neg.csv\"), index=False)  # this is ones that are not in rated_training_more_neg\n",
    "\n",
    "\n",
    "    for df, suffix in zip([df_train, df_valid], [\"training_more_neg\", \"validation_more_neg\"]):\n",
    "\n",
    "        sentence_list = []\n",
    "        ratings_list = []\n",
    "        avg_rating_list = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            sent = row[\"sentences\"]\n",
    "            ratings = np.array(row.drop(\"sentences\"))\n",
    "            sent = sent.replace(\"(|)\", \"\").replace(\"[]\", \"\")\n",
    "            sentence_list.append(sent)\n",
    "            avg_rating_list.append(transform_rating(ratings))\n",
    "            ratings_list.append(ratings)\n",
    "\n",
    "        ratings_list = np.array(ratings_list)\n",
    "        avg_rating_list = np.array(avg_rating_list)\n",
    "\n",
    "        f = open(os.path.join(path_, f\"{suffix}.json\"), \"w\")\n",
    "\n",
    "        for sent, rate in zip(sentence_list, avg_rating_list):\n",
    "\n",
    "            big_dict = {}\n",
    "            list_dict = []\n",
    "            list_dict.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            list_dict.append({\"role\": \"user\", \"content\": user_prompt + sent})\n",
    "            list_dict.append({\"role\": \"assistant\", \"content\": str(rate)})\n",
    "            big_dict[\"messages\"] = list_dict\n",
    "\n",
    "            line = str(json.dumps(big_dict)) + \" \\n\"\n",
    "            f.write(line)\n",
    "\n",
    "        f.close()\n",
    "    return [df_train, df_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = make_json(path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS BATCHING METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 156\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train), len(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 49999\n",
    "\n",
    "saved_data = []\n",
    "list_todo = []\n",
    "list_sentences = []\n",
    "df_both = pd.concat([df_train, df_valid], ignore_index=True)\n",
    "for index, row in df_both.iterrows():\n",
    "    if index not in saved_data:\n",
    "        list_todo.append(index)\n",
    "    list_sentences.append(row[\"sentences\"])\n",
    "\n",
    "\n",
    "list_of_batch = []\n",
    "for i in range(100):\n",
    "    if (len(list_of_batch) + 1) * batch_size < len(list_todo):\n",
    "        list_of_batch.append(list_todo[len(list_of_batch) * batch_size : (len(list_of_batch) + 1) * batch_size])\n",
    "    else:\n",
    "        list_of_batch.append(list_todo[len(list_of_batch) * batch_size :])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print([len(list_of_batch[i]) for i in range(len(list_of_batch))])\n",
    "# print(len(set(list_of_batch[0]).union(set(list_of_batch[1]))) / (2 * batch_size))  # needed only if we have 2+ batches\n",
    "print(len(list_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_todo in range(0, 1):\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for index_sent in list_of_batch[batch_todo]:\n",
    "\n",
    "        citation_text = list_sentences[index_sent]\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": f\"task-{batch_todo}-{index_sent}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                # \"model\": \"gpt-3.5-turbo-1106\",\n",
    "                \"model\": \"PUT_MODEL_NUM\",  # Trained model.\n",
    "                \"temperature\": 0.01,\n",
    "                \"messages\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt + citation_text}],\n",
    "            },\n",
    "        }\n",
    "        tasks.append(task)\n",
    "\n",
    "    file_name = os.path.join(path_, f\"sentiment_batch_{batch_todo}.jsonl\")  # Name the file.\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for obj in tasks:\n",
    "            file.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "    batch_file = client.files.create(file=open(file_name, \"rb\"), purpose=\"batch\")\n",
    "\n",
    "    print(batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(input_file_id=batch_file.id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
