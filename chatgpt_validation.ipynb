{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import openai\n",
    "import pickle\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "from irrCAC.raw import CAC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import external_methods as em\n",
    "from helper_functions import loadPKL, get_IRR, pval_star, savePKL, sorted_dict, print_sigfig\n",
    "\n",
    "CWD = os.path.abspath(\"\")  # Jupyter notebook path.\n",
    "\n",
    "dir_input = os.path.join(CWD, \"input\")\n",
    "dir_batch = os.path.join(CWD, \"batch\")  # ChatGPT related output.\n",
    "dir_TEMP = os.path.join(CWD, \"TEMP\")  # Intermediate files.\n",
    "dir_dict = os.path.join(CWD, \"dicts\")  # Look up dictionaries such as paper2meta; main data directory.\n",
    "dir_npy = os.path.join(CWD, \"npy\")  # Data files needed for plotting figures.\n",
    "dir_output = os.path.join(CWD, \"output\")  # Figures.\n",
    "dir_xml = os.path.join(CWD, \"xml\")  # xml files.\n",
    "dir_DEBUG = os.path.join(CWD, \"DEBUG\")\n",
    "\n",
    "dir_new = os.path.join(dir_batch, \"iter_all\")\n",
    "dir_new_files = os.path.join(dir_batch, \"iter_all\", \"files\")\n",
    "i, lab = 3, None\n",
    "dir_new_i = os.path.join(dir_batch, \"iter_all\", f\"i={i}\")\n",
    "ib, lab = 3, None\n",
    "dir_new_ib = os.path.join(dir_batch, \"iter_all\", \"bench\", f\"i={ib}\")\n",
    "model = \"gpt-4.1-mini-2025-04-14\"\n",
    "\n",
    "\n",
    "with open(os.path.join(dir_input, \"params.json\")) as f:\n",
    "    params = json.load(f)\n",
    "print(params)\n",
    "\n",
    "api_key = input()  # Run this cell and then enter your OpenAI api key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before proceeding\n",
    "\n",
    "external_methods.save_CGPT_input_files() should have been run already, generating two files:\n",
    "\n",
    "`sentences2rate-CGPT.txt` and `sentrow2edgeinfo.pkl` (in the `TEMP` folder).\n",
    "\n",
    "and cite_coauthor_functions.make_paper2meta() ran, generating `paper2meta.pkl` in `dicts` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sample from sentence dataset w/o replacement\n",
    "### Randomly sample from paper xml w/o replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once; for initial prompt guide.\n",
    "bk = em.sample_sentence_snippet(dir_TEMP, dir_xml, dir_batch, n_iter=3, n_samp=100, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once; generate all rand samples (w/o replacement).\n",
    "bk_all = em.sample_sentence_snippet(dir_TEMP, dir_xml, dir_dict, dir_new_files, n_iter=100, n_samp=3000, n_iter2=10, n_samp2=1000, seed=10)\n",
    "savePKL(dir_new_files, \"bk_all\", bk_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests to gpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~700(input)+100(output) tokens per request.\n",
    "fname = f\"sentences2rate-CGPT_{i}\"\n",
    "init_dict = em.CGPT_init_crit_5_explain(api_key)\n",
    "print(model)\n",
    "em.creat_batch_jobs_fc(init_dict, model, dir_new_files, dir_new_i, batch_size=3000, fname=fname, batch_num=None, bench=False)\n",
    "# em.send_request_nonbatch_fc(init_dict, model, dir_new_files, dir_new_i, fname=fname, i=i, lab=lab, interval=0.012, num=3000, bench=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~9K(input) tokens per request. (With 100K context window.)\n",
    "fname = f\"benchwork_text_CGPT_{ib}\"\n",
    "init_dict = em.CGPT_init_benchwork(api_key)\n",
    "print(model)\n",
    "em.creat_batch_jobs(init_dict, model, dir_new_files, dir_new_ib, batch_size=1000, fname=fname, batch_num=None)\n",
    "# em.send_request_nonbatch(init_dict, model, dir_new_files, dir_new_ib, fname=fname, i=ib, lab=lab, interval=0.012, num=1000, bench=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process gpt outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.process_batch_outputs_fc(dir_new_i, dir_new_i, i=i)\n",
    "row2rate0 = em.process_row2rate(loadPKL(dir_new_i, f\"gpt-4.1-mini-2025-04-14-row2rate_reason-{i}\"), verbose=False, fc=True, bench=False)\n",
    "Counter([v[0] for v in row2rate0.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.process_batch_outputs(dir_new_ib, dir_new_ib, i=ib)\n",
    "row2rate0 = em.process_row2rate(loadPKL(dir_new_ib, f\"gpt-4.1-mini-2025-04-14-row2rate-{ib}\"), verbose=False, fc=False, bench=True)\n",
    "Counter([v for v in row2rate0.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample balanced dataset to rate by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.subsample_save_sentence2rate_reason_csv(dir_new_i, dir_new_files, dir_new_i, i, model, n_samps={-1: 20, 1: 20, 0: 20}, seed=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.subsample_save_snippet2rate_reason_csv(dir_new_ib, dir_new_files, dir_new_ib, ib, model, n_samps={1: 20, 0: 20}, seed=1000 + ib, fc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate human judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(os.path.join(dir_new_i, f\"sentence2rate_reason_{i}_rate_em_px.xlsx\"), nrows=60)\n",
    "df2 = pd.read_excel(os.path.join(dir_new_ib, f\"snippet2rate_{ib}_rate_em_px.xlsx\"), nrows=40)\n",
    "\n",
    "for t, df in zip([\"Sentiment\", \"Benchwork\"], [df1, df2]):\n",
    "    df[[\"r1\", \"r2\", \"r3\"]] = df[[\"r1\", \"r2\", \"r3\"]].map(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "    df[\"consensus\"] = (df[[\"r1\", \"r2\", \"r3\"]].nunique(axis=1) == 1).astype(int)\n",
    "    ck_12 = cohen_kappa_score(df[\"r1\"], df[\"r2\"])\n",
    "    ck_13 = cohen_kappa_score(df[\"r1\"], df[\"r3\"])\n",
    "    ck_23 = cohen_kappa_score(df[\"r2\"], df[\"r3\"])\n",
    "    fk = fleiss_kappa(aggregate_raters(df[[\"r1\", \"r2\", \"r3\"]])[0], method=\"fleiss\")\n",
    "    a1 = CAC(df[[\"r1\", \"r2\", \"r3\"]], confidence_level=0.95).gwet()[\"est\"]\n",
    "    fk = CAC(df[[\"r1\", \"r2\", \"r3\"]], confidence_level=0.95).fleiss()[\"est\"]\n",
    "    df[[\"consensus\"]].to_csv(os.path.join(dir_new_i, \"consensus.csv\"))\n",
    "    print(f\"\\n{t} n={len(df)}:\")\n",
    "    print(f\"Cohen's Kappa: {ck_12:.3f}\", f\"{ck_13:.3f}\", f\"{ck_23:.3f}\")\n",
    "    avg = np.mean([np.mean(df['r1']=='YES')*100, np.mean(df['r2']=='YES')*100, np.mean(df['r3']=='YES')*100])\n",
    "    print(f\"Rater agreement % (avg={print_sigfig(avg)}): {print_sigfig(np.mean(df['r1']=='YES')*100)} {print_sigfig(np.mean(df['r2']=='YES')*100)} {print_sigfig(np.mean(df['r3']=='YES')*100)}\")\n",
    "    print(a1)\n",
    "    print(fk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(os.path.join(dir_new_i, f\"sentence2rate_reason_{i}_rate_em_px.xlsx\"), nrows=60)\n",
    "\n",
    "t = \"Sentiment\"\n",
    "\n",
    "df1[[\"r1\", \"r2\", \"r3\"]] = df1[[\"r1\", \"r2\", \"r3\"]].map(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "df1[\"consensus\"] = (df1[[\"r1\", \"r2\", \"r3\"]].nunique(axis=1) == 1).astype(int)\n",
    "for sent in [1, 0, -1]:\n",
    "    df = df1.loc[df1[\"LLM rating\"] == sent]\n",
    "    ck_12 = cohen_kappa_score(df[\"r1\"], df[\"r2\"])\n",
    "    ck_13 = cohen_kappa_score(df[\"r1\"], df[\"r3\"])\n",
    "    ck_23 = cohen_kappa_score(df[\"r2\"], df[\"r3\"])\n",
    "    fk = fleiss_kappa(aggregate_raters(df[[\"r1\", \"r2\", \"r3\"]])[0], method=\"fleiss\")\n",
    "    a1 = CAC(df[[\"r1\", \"r2\", \"r3\"]], confidence_level=0.95).gwet()[\"est\"]\n",
    "    fk = CAC(df[[\"r1\", \"r2\", \"r3\"]], confidence_level=0.95).fleiss()[\"est\"]\n",
    "    df[[\"consensus\"]].to_csv(os.path.join(dir_new_i, \"consensus.csv\"))\n",
    "    print(f\"\\n{t} {sent} n={len(df)}:\")\n",
    "    print(f\"Cohen's Kappa: {ck_12:.3f}\", f\"{ck_13:.3f}\", f\"{ck_23:.3f}\")\n",
    "    avg = np.mean([np.mean(df['r1']=='YES')*100, np.mean(df['r2']=='YES')*100, np.mean(df['r3']=='YES')*100])\n",
    "    print(f\"Rater agreement % (avg={print_sigfig(avg)}): {print_sigfig(np.mean(df['r1']=='YES')*100)} {print_sigfig(np.mean(df['r2']=='YES')*100)} {print_sigfig(np.mean(df['r3']=='YES')*100)}\")\n",
    "    print(a1)\n",
    "    print(fk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human agreed WET/DRY samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_nonce = os.path.join(dir_new, \"bench\", \"i=1-n=40-benchtest\")\n",
    "df = pd.read_excel(os.path.join(dir_nonce, \"snippet2rate_reason_1_rate_em_px.xlsx\"), nrows=40)\n",
    "pmc2wet = {int(x.split(\"PMC\")[1]): y for x, y in zip(df[\"URL\"], df[\"true_rating\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_nonce = os.path.join(dir_new, \"bench\", \"i=3\")\n",
    "df = pd.read_excel(os.path.join(dir_nonce, \"snippet2rate_3_rate_em_is.xlsx\"), nrows=40)\n",
    "pmc2rev = {int(x.split(\"PMC\")[1]): y for x, y in zip(df[\"URL\"], df[\"Column1\"]) if y == \"R\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
